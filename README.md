**Assista meu v√≠deo sobre a resolu√ß√£o do desafio:** https://www.youtube.com/watch?v=osf6MJqHQIQ

## Resolu√ß√£o Desafio - PICK 2024_01

Este reposit√≥rio cont√©m a minha resolu√ß√£o do [desafio proposto](https://github.com/badtuxx/LINUXtips-PICK-24_01) pelo [@Badtuxx](https://github.com/badtuxx) para a Turma do PICK 2024.
A ideia principal do desafio √© realizar a cria√ß√£o de um ambiente Kubernetes gerenciado e totalmente seguro para executar de maneira eficaz a aplica√ß√£o **Giropops Senhas**
A aplica√ß√£o **Giropops Senhas** √© uma ferramenta desenvolvida pela LinuxTips durante live na Twitch para ajudar os usu√°rios a criar senhas fortes e seguras de forma r√°pida e personalizada. Ela permite que voc√™ gere senhas aleat√≥rias, escolhendo o tamanho das senhas, e se far√° a inclus√£o de caracteres especiais e/ou n√∫meros.
## Sum√°rio desta Doc:

 - [Stack de Ferramentas](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#stack-de-ferramentas-tecnologias)
 - [Cria√ß√£o da Infraestrutura](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#stack-de-ferramentas-tecnologias)
 - [Diferentes formas para se criar uma imagem](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#diferentes-formas-para-se-criar-uma-imagem)
	 - [Primeiro M√©todo: O build convencional](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#primeiro-m%C3%A9todo-o-build-convencional) 
	 - [Segundo M√©todo: Build utilizando Imagens Distroless](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#segundo-m%C3%A9todo-build-utilizando-imagens-distroless)
	 - [Terceiro M√©todo: Vamos criar uma imagem do zero?](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#terceiro-m%C3%A9todo-vamos-criar-uma-imagem-do-zero)
 - [Criando Deploys com o Helm](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#criando-deploys-com-o-helm)
 - [Garantindo a seguran√ßa com Kyverno](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#garantindo-a-seguran%C3%A7a-com-kyverno)
 - [Utilizando o Harbor para garantir imagens seguras](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#utilizando-o-harbor-para-garantir-imagens-seguras)
 - [Monitorando nossa Aplica√ß√£o](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#monitorando-nossa-aplica%C3%A7%C3%A3o)
 - [Hora de Estressar!](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#hora-de-estressar)
	 - [Conhecendo o Kubernetes-based Event Driven Autoscaling - KEDA](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#conhecendo-o-kubernetes-based-event-driven-autoscaling---keda)
	 - [Stress Test com Locust](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#stress-test-com-locust)

Para resolver esse desafio, eu utilizei as seguintes ferramentas/ tecnologias:
### Stack de Ferramentas/ Tecnologias:

 - [APKO](https://github.com/chainguard-dev/apko)
 - [Distroless](https://edu.chainguard.dev/chainguard/chainguard-images/getting-started-distroless/)
 - [Docker](https://github.com/docker)
 - [Github Actions](https://github.com/features/actions)
 - [Harbor](https://github.com/goharbor/harbor)
 - [Helm](https://helm.sh/)
 - [KEDA](https://github.com/kedacore/keda)
 - [Kubernetes](https://kubernetes.io/pt-br/)
 - [Kyverno](https://kyverno.io/)
 - [Locust](https://locust.io/)
 - [Melange](https://github.com/chainguard-dev/melange)
 - [Metrics-Server](https://github.com/kubernetes-sigs/metrics-server)
 - [Nginx-Ingress](https://github.com/kubernetes/ingress-nginx)
 - [Prometheus Stack (+ Grafana)](https://github.com/prometheus-community/helm-charts)
 - [Terraform](https://www.terraform.io/)

### Cria√ß√£o da Infraestrutura
Eu me baseei em outro projeto que desenvolvi ainda durante o PICK2024 para a cria√ß√£o da infraestrutura da minha aplica√ß√£o, o [k8s-with-ec2](https://github.com/FabioBartoli/k8s-with-ec2). A ideia deste projeto √© criar um cluster kubernetes na AWS para estudos, provisionando e configurando m√°quinas EC2 pequenas utilizando M√≥dulos Terraform + Github Actions, de maneira que esse cluster seja gratuito utilizando o [free tier](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all) da AWS. O projeto √© aberto e voc√™ pode fazer um fork e configurar suas pr√≥prias credenciais para utiliz√°-lo :) Mais detalhes est√£o dispon√≠veis na documenta√ß√£o do pr√≥prio projeto.
Mas, para esse desafio, as coisas s√£o um pouco diferentes e, apesar de utilizar o mesmo "esqueleto" do Terraform, tive que fazer v√°rias altera√ß√µes para que o cluster criado suporte tudo que eu utilizarei, inclusive sendo necess√°rio utilizar m√°quinas fora do free tier para que atendam aos requisitos m√≠nimos do Kubernetes. 
Eu tamb√©m adicionei em meu Terraform o provisionamento de um Application LoadBalancer e de registros DNS na minha Hosted Zone para apontarem para este LoadBalancer que estarei criando. Aqui, eu tentei "contornar" a utiliza√ß√£o do EKS + Network Load Balancer, 2 recursos que possuem custos e n√£o est√£o inclusos no free tier, pela utiliza√ß√£o de inst√¢ncias EC2 gerenciadas pelo Kubeadm com um Application Load Balancer, que possui um per√≠odo de 750 horas/ m√™s para utiliza√ß√£o no Free Tier.
Em resumo, o cen√°rio que eu quis montar foi para pagar o menos poss√≠vel, em comparativo:
| Stack "Normal" - EKS + Inst√¢ncias + NLB             | Pre√ßo            | Stack "Manual", EC2 + ALB     | Pre√ßo                                  |
|-----------------------------------------------------|------------------|------------------------------------------------|----------------------------------------|
| EKS                                                 | $0,10/hora        | 4 Inst√¢ncias (1 CP + 3 Workers) - t3a.small     | (0,0188) * 4 = $0,0752/hora             |
| NLB                                                 | $0,0225/hora      | Application Load Balancer                       | 750 horas/m√™s gr√°tis no Free Tier      |
| 4 Inst√¢ncias (1 CP + 3 Workers) - t3a.small         | (0,0188) * 4 = $0,0752/hora | |                                            |
| **Total**                                           | **$0,1977/hora**  | **Total**                                               |  **$0,0752/hora**                                       |  


O custo de se utilizar a Stack "Normal", se considerar que eu utilizaria 750 horas/ m√™s, seria de **$148,27 d√≥lares**, enquanto utilizando a stack onde s√≥ estou pagando pelas EC2, ser√° de **$56,40 d√≥lares**, uma economia de **mais de 60%**
Claro que essa utiliza√ß√£o de um Cluster "manual" possui diversos problemas e √© ideal apenas para ambiente de estudos, para pouparmos a carteira üòä. No "mundo real", utilizo o EKS sem pensar duas vezes.
Voltando para a infraestrutura, o "grande truque" para se utilizar o Application Load Balancer √© justamente o Nginx Ingress. Ao inv√©s de cri√°-lo com o tipo default para cloud "LoadBalancer", eu estou criando nos padr√µes da utiliza√ß√£o Bare Metal, criando um servi√ßo "NodePort" e bindando 2 portas:

 - 30080 da m√°quina -> 80 do Ingress
 - 30443 da m√°quina -> 443 do Ingress

Quem vai fazer a m√°gica acontecer √© o LoadBalancer: Eu tenho um listener manda as requisi√ß√µes da porta 80 em redirect para a 443, e as requisi√ß√µes da 443 eu encaminho para meu cluster EC2 usando um TargetGroup, diretamente para a porta 30443. E pra melhorar, como meu Application Load Balancer possui um certificado v√°lido na porta 443, as aplica√ß√µes que forem expostas pelo Ingress ir√£o automaticamente j√° ter a confian√ßa do meu certificado. 
No desenho abaixo eu tento explicar um pouco melhor tudo que foi feito:

![Arquitetura-Cluster](./docs/images/Arquitetura-Infra.png)

Para fazer o deploy da minha infra, eu tenho um [Pipeline CI/CD diretamente no meu Github Actions](./.github/workflows/deploy-infra.yml), e tudo que eu preciso para execut√°-lo s√£o esses 4 valores:

![Keys-AWS](./docs/images/AWS-keys.png)

 - *ACCESS_KEY*: Access Key de um usu√°rio do IAM que tenha permiss√£o na AWS para criar e gerenciar inst√¢ncias, load balancers, buckets s3 e rotas no route53
 - *SECRET_KEY*: A secret desse usu√°rio
 - *PRIVATE_KEY* e *PUBLIC_KEY* s√£o literalmente uma key gerada em minha m√°quina local, codificadas em base64 e salvas como secrets no Github. Essas chaves ser√£o utilizadas pelo Terraform para vincular nas EC2 e depois fechar um t√∫nel SSH para fazer a instala√ß√£o autom√°tica do meu Cluster Kubernetes, al√©m da instala√ß√£o do Helm no meu Worker, adi√ß√£o dos Repos Helm e afins.
Infra criada, vamos ver um pouco sobre a cria√ß√£o da Imagem
### Diferentes formas para se criar uma Imagem
A cria√ß√£o de um Dockerfile √© uma pr√°tica bastante comum quando pensamos em colocar nossas aplica√ß√µes para serem executadas em ambientes com orquestra√ß√£o. Temos diversas maneiras de gerar uma imagem como o pr√≥prio [Docker](https://www.docker.com/) que acabei de citar, o [Podman](https://podman.io/), e at√© mesmo a cria√ß√£o das nossas imagens "do zero". Nesse desafio, eu tenho 3 reposit√≥rios com a exata mesma aplica√ß√£o onde eu irei aplicar t√©cnicas de build da imagem diferente, para que possamos comparar os benef√≠cios de cada uma.
#### Primeiro M√©todo: O build convencional
[Nessa pasta](./app/normal-giropops-senhas), voc√™ vai encontrar o build mais convencional que costumamos fazer: pega uma imagem base, instalamos o que for necess√°rio, definimos o entrypoint da nossa aplica√ß√£o e sucesso! √â uma forma v√°lida de se fazer um deploy, mas vamos olhar para alguns dados:

![build-normal](./docs/images/build-normal.png)

Ent√£o, pegamos uma imagem base do python, fizemos todo o processo que precis√°vamos para o build e construimos uma bela imagem de 148MB. No fim, temos uma imagem com **23 camadas** com o Debian sendo a imagem base:

![camadas-build](./docs/images/camadas-buildnormal.png)

Perceba que o grande problema aqui √© que, caso essa nossa imagem sofra um vazamento ou algo do tipo, qualquer pessoa poder√° recuperar qualquer conte√∫do que tenha nessas camadas, mesmo que ele tenha sido apagado antes do "build do container final". Al√©m disso, estamos expostos a quaisquer vulnerabilidades que venham a surgir em pacotes nativos do sistema operacional base ou relacionados a alguma ferramenta que um dev venha a incluir nesse nosso Dockerfile. Vamos utilizar o Trivy, uma ferramenta de scan de vulnerabilidades para ver qual o "tamanho do problema" que temos em m√£os:

![scan-build](./docs/images/scan-build-normal.png)

Ent√£o, temos 105 vulnerabiliadades encontradas, incluindo **1 CR√çTICA**, e dessas 105, apenas 1 possui um fix dispon√≠vel, fix que com certeza ser√° ou j√° foi lan√ßado em uma imagem mais recente do python no DockerHub, mas mesmo assim ainda temos bastante coisa. Ent√£o, vamos olhar para o segundo cen√°rio

#### Segundo M√©todo: Build utilizando Imagens Distroless
Aqui, a ideia √© ter imagens que tem apenas os pacotes necess√°rios para rodar nossa aplica√ß√£o, ent√£o, toda a parte dos pacotes do Sistema Operacional e etc que j√° discutimos antes j√° n√£o s√£o mais um problema pra gente. Com as imagens Distroless conseguimos ter mais confiabilidade nas nossas imagens criadas. Voc√™ pode [verificar aqui](./app/distroless-giropops-senhas/Dockerfile) como utilizei o distroless para criar o Dockerfile do giropops-senhas. Em resumo, utilizamos uma imagem base "builder" com um sistema operacional para fazer todo o processo de build da nossa aplica√ß√£o e depois copiamos apenas os execut√°veis necess√°rios para uma imagem do tipo [APKO](https://edu.chainguard.dev/open-source/build-tools/apko/getting-started-with-apko/) usando o conceito de [Docker Multi-Stage](https://docs.docker.com/build/building/multi-stage/)
Agora, vamos verificar como ficou nossa imagem:

![build-distroless](./docs/images/build-distroless.png)

Perceba que o processo de build ficou um pouco mais complexo, mas j√° temos uma primeira mudan√ßa significante: Para a mesma aplica√ß√£o, conseguimos diminuir para 72MB o tamanho da imagem. Praticamente, cortamos a primeira imagem gerada pela metade
E olhando para as camadas da imagem, tamb√©m conseguimos perceber que elas reduziram significativamente: agora temos 11 camadas. Todo o processo realizado pelo container "builder" ficou contido dentro de uma √∫nica camada base chamada de "apko".

![camadas-distroless](./docs/images/camadas-builddistroless.png)

Outro ponto bem interessante de se notar √© que agora eu n√£o consigo mais executar o "cat /etc/os-release" para pegar o meu sistema operacional, pois esse pacote n√£o vem instalado em nossa imagem final. A n√£o presen√ßa de pacotes como *cat*, *wget*, *curl* em nossa imagem final √© bem interessante quando olhamos para a seguran√ßa, visto que diminui a superf√≠cie de ataque em nossa imagem. Por fim, vamos verificar as vulnerabilidades pra ver o que temos por aqui..

![scan-distroless](./docs/images/scan-build-distroless.png)

0 vulnerabilidades listadas! Isso mostra o trabalho da Chainguard em manter as imagens atualizadas com os pacotes mais recentes poss√≠veis! J√° temos algo bem interessante agora, certo? Mas conseguimos melhorar mais um pouco

#### Terceiro M√©todo: Vamos criar uma imagem do zero?
![Melange](https://64.media.tumblr.com/9d29acae84f6e499a6baffc6ecd2cc9c/tumblr_oyjhay1HcD1qmob6ro1_400.gif)

Aqui, veremos um outra op√ß√£o muito interessante que a Chainguard nos proporciona: ainda que utilizemos uma imagem base deles, podemos precisar de algo muito espec√≠fico pra nossa aplica√ß√£o executar corretamente e iremos acabar instalando em nossa imagem final. O resultado, como j√° sabemos, s√£o mais camadas e mais possibilidades de termos pacotes que tenham depend√™ncias com vulnerabilidades.
N√£o s√≥ para esse cen√°rio do exemplo, mas para qualquer outro, a seguinte alternativa pode ser interessante: que tal construirmos uma imagem do zero com os pacotes que precisamos especificamente para nossa aplica√ß√£o e somente isso? Logicamente, estamos aumentando mais um pouco o grau de complexidade do nosso build, mas teremos melhores recompensas.
Para fazer esse processo, a Chainguard nos disponibiliza o [Melange](https://github.com/chainguard-dev/melange) ~~(n√£o √© a especiaria de Duna, √© outro Melange)~~, uma ferramenta que permite criarmos nossa aplica√ß√£o como um "pacote" junto com suas depend√™ncias necess√°rias e fazer o build disso. Para efeitos de entendimento, nosso processo de cria√ß√£o da imagem fica dividido entre:

 - [Criar o nosso pacote Melange](./security/melange/melange.yaml): Primeiro, vamos buildar nossa aplica√ß√£o como um pacote com todas as depend√™ncias necess√°rias, e nada mais que isso
 - [Colocar nosso pacote para rodar em uma imagem APKO](./security/melange/apko.yaml): Copiamos nosso aplica√ß√£o como um pacote para dentro de outra imagem, definimos o seu execut√°vel como entrypoint e temos nossa aplica√ß√£o funcionando maravilhosamente

Dito isto, vou fazer o build da nossa imagem para ver o que acontece: 

![build-melange](./docs/images/build-melange.png)

Podemos ver que a complexidade aumentou mais um pouco, pois agora √© gerado um arquivo compactado da nossa aplica√ß√£o que carregamos para o docker para poder utilizar. Mas j√° conseguimos diminuir para 59MB (a primeira tinha 148MB), agora sabemos que a imagem tem apenas o que precisamos para executar nossa aplica√ß√£o. Mas agora, a cereja do bolo:

![camadas-melange](./docs/images/camadas-buildmelange.png)

Nossa imagem tem APENAS UMA CAMADA apko. O que significa que todo nosso processo de buildar a imagem agora fica abstra√≠do e nenhuma informa√ß√£o sobre isso pode ser descoberto na imagem final. Perceba tamb√©m que agora o meu comando "docker run" com o "cat /etc/os-release" nem foi respeitado, pois nosso ponto de acesso √© justamente a nossa aplica√ß√£o, sem muitas possibilidades para cair dentro do container. Apenas para n√£o restar d√∫vidas, vamos rodar o scan nela tamb√©m:
![scan-melange](./docs/images/scan-build-melange.png)

Tamb√©m n√£o temos vulnerabilidades. Para o restante deste desafio, eu irei trabalhar com a imagem do giropops-senhas criada via Melange
##
### Criando Deploys com o Helm
O Helm √© um gerenciador de pacotes desenvolvido para facilitar o provisionamento de aplica√ß√µes no Kubernetes. Assim como em um sistema operacional n√≥s temos os gerenciadores, como por exemplo o "apt", o Helm tem quase a mesma fun√ß√£o para o Kubernetes. Aqui, a ideia √© que consigamos aplicar a mesma exata configura√ß√£o para diferentes ambientes, alterando somente o que for necess√°rio atrav√©s do "values"
Durante esse laborat√≥rio, eu instalei os seguintes pacotes Helm:

    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo add harbor https://helm.goharbor.io
    helm repo add kyverno https://kyverno.github.io/kyverno/
    helm repo add kedacore https://kedacore.github.io/charts

Todos esses pacotes s√£o criados pelos pr√≥prios mantenedores das aplica√ß√µes ou pela comunidade, com o intuito de tornar essas ferramentas f√°ceis de serem "instaladas" dentro do seu cluster. Mas tamb√©m podemos criar os pr√≥prios pacotes para a nossa aplica√ß√£o.
No meu caso, tenho duas stacks que podem tirar bom proveito dessa configura√ß√£o compartilhada: os Ingress que eu irei criar para cada endpoint que ficar√° acess√≠vel, conforme eu demonstrei na [imagem](https://github.com/FabioBartoli/LINUXtips-PICK?tab=readme-ov-file#cria%C3%A7%C3%A3o-da-infraestrutura) acima, e tamb√©m o giropops-senhas, que √© composto por Aplica√ß√£o + Redis.
Os dois Helms est√£o publicados aqui neste reposit√≥rio mesmo e voc√™ pode conferir nos seguintes links:
[Helm Chart do Giropops-App](https://github.com/FabioBartoli/LINUXtips-PICK/tree/main/manifests/helm/giropops-app)
[Helm Chart do Ingress](https://github.com/FabioBartoli/LINUXtips-PICK/tree/main/manifests/helm/ingress)
A estrutura do meu Helm √© basicamente a seguinte:

![estrutura-helm](./docs/images/estrutura-helm.png)

 - Chart.yaml - Defini√ß√£o de vers√£o do Helm, aplica√ß√£o, descri√ß√£o e etc
 - Pasta "templates" - Onde ficam os arquivos que ser√£o consumidos para a cria√ß√£o dos meus recursos
 - values.yaml: defini√ß√£o dos valores que ser√£o inclusos em cada um dos recursos criados

Para fazer a instala√ß√£o do Helm criado no meu Cluster, executei os seguintes comandos:

    ## Incluir os repos:
    helm repo add ingress https://fabiobartoli.github.io/LINUXtips-PICK/manifests/helm/ingress/
    helm repo add giropops-app https://fabiobartoli.github.io/LINUXtips-PICK/manifests/helm/giropops-app/
    # Helm Update:
    helm repo update
    # Instalar os pacotes:
    helm  install  ingress-controller  ingress/ingress-templates
    helm install giropops giropops-app/giropops-chart

No giropops-senhas, para simular uma instala√ß√£o multi-ambiente, eu coloquei uma label chamada "env" que pode ser passada diretamente no comando de instala√ß√£o do Helm. Se nada for passado, ela subir√° como "dev". Se eu passar algum par√¢metro, ir√° respeitar o que eu passei. Se eu subir com a env "stg", consigo verificar que ela realmente est√° aqui:

![set-env](./docs/images/set-env.png)

Na pr√°tica, isso poderia ser utilizado em diversos campos dentro do meu template do Helm, como para defini√ß√£o de quantidade de recursos alocados a depender do ambiente, por exemplo.
Mas antes de fazer o deploy da aplica√ß√£o, precisamos configurar mais algumas coisas.

### Garantindo a seguran√ßa com Kyverno
O Kyverno √© uma ferramenta que nos permite criar PolicyAsCode no nosso cluster Kubernetes. Com ela, conseguimos criar pol√≠ticas declarativas para definir o que pode e o que n√£o pode ser feito em nosso cluster. Podemos criar regras para o cluster como um todo "ClusterPolicy" ou apenas para um namespace espec√≠fico "Policy". Na [documenta√ß√£o oficial](https://kyverno.io/docs/) do Kyverno, voc√™ pode ver diversos exemplos de pol√≠ticas que s√£o aplic√°veis no cluster
No meu cen√°rio, eu criei as seguintes pol√≠ticas:

 - [disallow-root-user.yaml](./security/kyverno/disallow-root-user.yaml) - N√£o permite que nenhum container criado execute como o usu√°rio root do container, com exce√ß√£o dos containers criados nos namespaces das ferramentas que irei utilizar (kyverno, ingress-nginx, prometheus, por exemplo)
 - [disalow-default-ns.yaml](./security/kyverno/disalow-default-ns.yaml) - N√£o permite que nenhum container suba utilizando o namespace default do cluster, sem exce√ß√µes
 - [allow-only-harbor-registry.yaml](./security/kyverno/allow-only-harbor-registry.yaml) - Permite que eu utilize apenas imagens que venham do meu registry Harbor
 - [harbor-signature.yaml](./security/kyverno/harbor-signature.yaml) - Al√©m de permitir apenas do meu registry privado com a regra acima, essa verifica se a imagem foi assinada pelo Cosign utilizando uma chave v√°lida
 - [require-probes.yaml](./security/kyverno/require-probes.yaml) - N√£o permite que nenhum container suba sem as probes definidas, com exce√ß√£o dos containers criados nos namespaces das ferramentas que irei utilizar
 - [verify-sensitive-vars.yaml](./security/kyverno/verify-sensitive-vars.yaml) - N√£o permite que secrets sejam montadas como env dentro dos containers

Agora vamos testar essas pol√≠ticas:
Primeiro, vou criar uma secret gen√©rica apenas para fazer o teste de pass√°-la como vari√°vel:

    kubectl create secret generic minha-secret --from-literal=strigus=girus

E vamos criar um deploy totalmente gen√©rico que tenta montar essa secret:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:latest
            env:
            - name: MINHA_SECRET
              valueFrom:
                secretKeyRef:
                  name: minha-secret
                  key: chave
            ports:
            - containerPort: 80

O resultado:

![kyverno-bloqueios](./docs/images/kyverno-bloqueios.png)

J√° de cara, fomos bloqueados por 5 pol√≠ticas diferentes, pois nosso deployment n√£o est√° nos padr√µes permitidos para o cluster. Vamos ajustar as 5 pol√≠ticas para verificar se dar√° tudo certo:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment
      namespace: deploy-inseguro
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: harbor.fabiobartoli.com.br/pick2024/nginx-assinada
            ports:
            - containerPort: 80
            securityContext:
              runAsUser: 1000
              runAsGroup: 1000
              allowPrivilegeEscalation: false
            livenessProbe:
              httpGet:
                path: /
                port: 80
              initialDelaySeconds: 10
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /
                port: 80
              initialDelaySeconds: 5
              periodSeconds: 5
          imagePullSecrets:
            - name: regcred


Perceba que eu estou passando agora uma imagem que realmente est√° dispon√≠vel no meu registry privado, passando minha credencial para logar no registry privado e, ainda mais, est√° sim assinada:

![nginx-assinada](./docs/images/nginx-assinada.png)

S√≥ tem um problema... Eu assinei essa imagem com uma chave falsa! Uma chave que n√£o possui rela√ß√£o de confian√ßa com minha chave p√∫blica cadastrada na pol√≠tica "require-harbor-signature". Vamos ver se seremos identificados:

![false-sig](./docs/images/false-signature.png)

E sim, todas nosssas pol√≠ticas est√£o funcionando maravilhosamente bem! Apenas pra n√£o deixar passar, vou ajustar a assinatura dessa imagem e tentar realizar a cria√ß√£o do deployment novamente:

![image-adjust](./docs/images/imagem-ajustada.png)

Nossas roles est√£o funcionando bem, ent√£o agora podemos finalmente fazer o deployment do Giropops-Senhas em si

### Utilizando o Harbor para garantir imagens seguras
Como j√° disse anteriormente, nessa stack que criei eu utilizei o Harbor como ferramenta para reposit√≥rios privados. O Harbor n√£o estava incluso nos requisitos para o desafio, mas √© uma ferramenta muito √∫til que me foi apresentada no PISC pelo professor [P0ssuidao](https://github.com/P0ssuidao) e que eu j√° estou utilizando no meu dia a dia.
Basicamente, o Harbor serve para armazenarmos nossas imagens de container, mas possui muitas outras fun√ß√µes que j√° facilitam demais a garantia de seguran√ßa nas imagens. Para esse desafio, eu configurei no Harbor os seguintes par√¢metros:
 - Todas as imagens que eu fa√ßo push para o Harbor s√£o automaticamente scanneadas pelo Trivy da Aquasecurity e o relat√≥rio fica dispon√≠vel para mim
 - Qualquer imagem que estiver dispon√≠vel no Harbor, s√≥ poder√° ser utilizada por algu√©m com acesso, se ela n√£o tiver NENHUMA vulnerabilidade dos n√≠veis "LOW" para cima
 - Qualquer imagem que estiver dispon√≠vel no Harbor, s√≥ poder√° ser utilizada por algu√©m com acesso, se ela estiver assinada utilizando o Cosign. Essa configura√ß√£o junto com a configura√ß√£o de verifica√ß√£o da assinatura me fazem ter confiabilidade que minha imagem final √© realmente a que eu construi, e n√£o uma modificada.
Abaixo est√° o print das minhas configura√ß√µes:

![harbor-configs](./docs/images/harbor-configs.png)

Com isso, tenho a garantia que minhas imagens da aplica√ß√£o sempre estar√£o assinadas pelo **Cosign** e ter√£o suas vulnerabilidades scanneadas pelo **Trivy**. Vamos partir para o deploy da imagem ent√£o.
Eu criei um pipeline CI/CD que √© respons√°vel pelo [build da imagem](./.github/workflows/build-image.yml) utilizando o Melange + APKO + Cosign + Harbor. Ent√£o, a ideia √© que o desenvolvedor fa√ßa as altera√ß√µes na sua aplica√ß√£o que est√° dentro da pasta [app/melange-giropops-senhas](./app/melange-giropops-senhas), fa√ßa o commit para a branch desejada e rode a esteira de build para a constru√ß√£o da imagem no Harbor. O meu pipeline ser√° respons√°vel por:

 - Definir o ambiente baseado na branch: Para que possamos ter imagens
   de dev, stg e prd, o pipeline definir√° o ambiente baseado na seguinte
   l√≥gica: 
	 - Rodou o pipeline na branch "master", ser√° buildada a imagem de produ√ß√£o (prd)
	 - Rodou o pipeline na branch "staging", ser√° buildada a imagem de homologa√ß√£o/qa (stg)
	 - Rodou o pipeline em qualquer outra branch, ser√° buildada a imagem de desenvolvimento (dev)
 - Montar as chaves para assinatura do Melange e do Cosign: Eu salvei como secrets em base64 no meu Github as chaves para montar a imagem do Melange e tamb√©m para assinar a imagem final utilizando o Cosign. Essas s√£o as secrets configuradas:
 
 ![melange-keys](./docs/images/melange-keys.png)
 
 - Realizar o processo de build do pacote com o Melange e build da imagem final via APKO: Para esse processo, eu estou utilizando containers com as imagens do Melange e do APKO por uma quest√£o de facilidade, mas eu poderia instalar esses pacotes no meu runner do Github Actions se assim preferisse
 - Fazer o "docker login" no Harbor e a instala√ß√£o do Cosign para assinatura da imagem
 - Realizar o tagueamento da imagem buildada, subir para o Harbor e fazer a assinatura dela
	 - Nesse step, eu tenho uma condicional para taguear a imagem como "latest" apenas se o build estiver sendo executado a partir da branch master, ou seja, o build produtivo

Agora sim, depois de executar o build e ter a imagem dispon√≠vel no Harbor, podemos executar o Helm do nosso giropops-senhas! Manualmente eu tamb√©m subi uma imagem do Redis disponibilizada pela Chainguard e fiz a assinatura dela. Podemos conferir os dados no nosso registry:

**giropops-senhas:**

![giropops-image](./docs/images/giropops-image.png)

**redis:**

![redis-image](./docs/images/redis-image.png)

Instalando o nosso Helm:

![enter image description here](./docs/images/helm-install-giropops.png)
   
E por fim, acessando nosso ambiente, com certificado seguro:

 ![acesso-giropops](./docs/images/acesso-giropops.png)

### Monitorando nossa Aplica√ß√£o
Quase chegando no fim da constru√ß√£o do ambiente, vamos falar agora sobre monitoramento. Realizei a instala√ß√£o da Stack Prometheus + Grafana no namespace "monitoring" do meu cluster com o comando abaixo:

    helm  install  kube-prometheus  prometheus-community/kube-prometheus-stack

Na aplica√ß√£o giropops-senhas, eu fiz algumas adi√ß√µes no c√≥digo de pontos que eu gostaria de monitorar. Voc√™ pode conferir em: [app.py](./app/melange-giropops-senhas/app.py)
Um resumo do que coloquei a mais para ser monitorado:

 - `senha_gerada_numeros`: Quero contar todas as senhas geradas que cont√©m n√∫meros
 - `senha_gerada_sem_numeros_counter`: Das senhas geradas, quantas n√£o possuem n√∫meros
 - `senha_gerada_caracteres_especiais`: Quero contar todas as senhas geradas que cont√©m caracteres especiais
 - `senha_gerada_sem_caracteres_especiais_counter`: Das senhas geradas, quantas n√£o possuem caracteres especiais
 - `redis_connection_error_counter`: Contador de erros de conex√£o com Redis
 - `tempo_gerar_senha`: Tempo que minha aplica√ß√£o demora para responder com uma senha gerada
 - `tempo_resposta_api`: Tempo de resposta da API
 - `api_errors`: Contador de erros de API
 - `tamanho_fila_senhas`: Tamanho da fila de senhas no Redis que a aplica√ß√£o est√° mantendo

Para monitorar o path /metrics da minha aplica√ß√£o, eu criei o ServiceMonitor que voc√™ pode [conferir aqui](./monitoring/prometheus-metrics.yaml)

![prom-target](./docs/images/prom-target.png)

Usando o Grafana vinculado ao Prometheus, eu montei o seguinte gr√°fico para monitorar a utiliza√ß√£o da minha aplica√ß√£o:

![grafana-dash](./docs/images/grafana-giropops.png)
![grafana-tempo-res](./docs/images/grafana-temporesposta.png)

Tamb√©m usei o Grafana para configurar um Alerta diretamente para o meu Telegram atrav√©s de um Bot no caso do deployment da aplica√ß√£o escalar para mais de 6 r√©plicas:

![grafana-alert](./docs/images/grafana-alert.png)
 
 Escalando meu deployment manualmente para 10 r√©plicas:
![scale](./docs/images/scale-deployment.png)

As mensagens j√° come√ßam a chegar no meu Telegram:

![bot-firing](./docs/images/bot-firing.png)

E quando o problema √© resolvido:

![bot-resolved](./docs/images/bot-resolved.png)

### Hora de Estressar!
Agora que a aplica√ß√£o j√° est√° sendo devidamente monitorada, chegou a hora de criar uma pol√≠tica de AutoScaling para meu deployment e fazer uma carga de estresse nele!
Para isso, primeiramente vou precisar do [metrics-server](https://github.com/kubernetes-sigs/metrics-server) instalado no meu cluster, e por conta da minha instala√ß√£o manual do kubernetes, eu preciso baixar o manifesto e passar o par√¢metro "`--kubelet-insecure-tls`" para que o deployment suba corretamente. Ent√£o, posso aplicar ele no meu cluster com o comando: 

    kubectl  apply  -f  /home/ubuntu/LINUXtips-PICK/manifests/metrics-hpa/metrics-components.yaml

Metrics Server instalado, podemos aplicar tamb√©m a pol√≠tica que criei para o Horizontal Pod Autoscaling do meu deployment:

    kubectl  apply  -f  /home/ubuntu/LINUXtips-PICK/manifests/metrics-hpa/hpa-giropops.senhas.yaml

Agora, antes de irmos para o stress test em si, tem outra coisa que precisamos notar na aplica√ß√£o: Se eu fizer um scaling do meu deployment do giropops-senhas sem mexer na minha quantidade de pods do redis, o meu servi√ßo n√£o comporta, e os containers come√ßam a dar "CrashLoopBackOff":

![crashloop](./docs/images/crashloop.png)

Precisamos garantir alguma estrat√©gia para que o Deployment do redis acompanhe, de algum modo, o ScalingUp e ScalingDown do Deployment do giropops-senhas... mas como podemos fazer isso de uma forma automatizada?

#### Conhecendo o Kubernetes-based Event Driven Autoscaling - KEDA
O KEDA √© um projeto reconhecido pela CNCF e que nos permite fazer Scaling de workloads baseados em eventos. Ent√£o, junto com o nosso Prometheus, por exemplo, conseguimos for√ßar o redis-deployment a fazer AutoScaling a medida que a aplica√ß√£o giropops-senhas tamb√©m fizer
A instala√ß√£o do KEDA tamb√©m √© bem simples no meu cluster, sendo:

    helm repo add kedacore https://kedacore.github.io/charts
    helm repo update
    helm  install  keda  kedacore/keda  --namespace  keda  --create-namespace

Com ele devidamente provisionado, eu posso aplicar a pol√≠tica que criei para o Scaling do redis-deployment:

    apiVersion: keda.sh/v1alpha1
    kind: ScaledObject
    metadata:
      name: redis-scaledobject
      namespace: giropops
    spec:
      scaleTargetRef:
        kind: Deployment
        name: redis-deployment
      minReplicaCount: 1
      maxReplicaCount: 10
      cooldownPeriod:  60
      triggers:
      - type: prometheus
        metadata:
          serverAddress: http://kube-prometheus-kube-prome-prometheus.monitoring.svc.cluster.local:9090/
          metricName: giropops_senhas_replicas
          threshold: '3'
          query: |
            sum(kube_deployment_status_replicas{namespace="giropops",deployment="giropops-senhas"})

Com essa pol√≠tica, eu estou dizendo que, para cada 3 r√©plicas do deployment do giropops-senhas, o KEDA deve provisionar 1 deployment do redis. Assim, a minha carga de trabalho ir√° escalar de maneira que suporte a demanda da aplica√ß√£o:

![keda-up](./docs/images/keda-working.png)

O contr√°rio tamb√©m ir√° acontecer: Assim que o HPA diminuir a necessidade de pods do nosso Deployment giropops-senhas, o KEDA tamb√©m ir√° diminuir a quantidade de r√©plicas do redis-deployment:

![keda-down](./docs/images/keda-scaledown.png)

Agora sim, podemos rodar nosso Stress Test!

### Stress Test com Locust
O Locust √© uma ferramenta OpenSource que permite executarmos carga de estresse em nossa aplica√ß√£o. Com ela, conseguimos inclusive, criar scripts python para definir em quais paths queremos bater e o que queremos realizar dentro da aplica√ß√£o
Voc√™ pode ver como o meu Locust est√° provisionado nos manifestos [dessa pasta](./manifests/locust)
Basicamente eu estou gerando 2 testes:

 - Ficar gerando senhas na API a partir da rota `/api/gerar-senha`
 - Ficar consultando as senhas atrav√©s da rota `/api/senhas`

Atrav√©s da interface web, vou fazer o Locust bater diretamente no Service do Giropops-Senhas. Como os dois servi√ßos est√£o dentro do Cluster, farei isso pelo DNS interno para que eu n√£o tenha problemas de carga na minha m√°quina ou ALB:

![locust-test](./docs/images/locust-test.png)

E ap√≥s executar os testes, estes foram os resultados que eu obtive:
O meu HPA escalou conforme esperado, assim com o KEDA:
![keda-hpa](./docs/images/get-top.png)

Grande parte das requisi√ß√µes passaram, mas ap√≥s uma certa carga, a minha p√°gina /api/gerar-senha come√ßou a retornar "Internal Server Error", mesmo a senha continuando a ser gerada

![statics](./docs/images/locust-statics.png)

![enter image description here](./docs/images/locust-charts.png)

Olhando no Grafana:

![grafana1](./docs/images/locust-grafana1.png)

![grafana2](./docs/images/locust-grafana2.png)

##
Com isso eu encerro a entrega do meu projeto do desafio e fico √† disposi√ß√£o caso algu√©m queira entender melhor algum dos pontos que eu passei aqui, e tamb√©m ficarei muito feliz caso possam contribuir me explicando coisas que eu esteja fazendo de maneira errada e qual seria a melhor alternativa.
Entrem em contato atrav√©s do meu Telegram ou LinkedIn:

<a href="https://www.linkedin.com/in/fabiobartoli/"><img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn"></a>
<a href="https://t.me/FabioBartoli"><img src="https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white" alt="Telegram"></a>


##
Esse projeto foi desenvolvido como parte do PICK - Programa Intensivo de Containers e Kubernetes - Turma 2024. [Saiba mais sobre](https://linuxtips.io/pick-2024/)
